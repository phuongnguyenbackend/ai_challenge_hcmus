{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from basenet.vgg16_bn import vgg16_bn, init_weights\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    def __init__(self, in_ch, mid_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch + mid_ch, mid_ch, kernel_size=1),\n",
    "            nn.BatchNorm2d(mid_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class CRAFT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CRAFT, self).__init__()\n",
    "        self.basenet = vgg16_bn(pretrained=False, freeze=False)\n",
    "        self.upconv1 = double_conv(1024, 512, 256)\n",
    "        self.upconv2 = double_conv(512, 256, 128)\n",
    "        self.upconv3 = double_conv(256, 128, 64)\n",
    "        self.upconv4 = double_conv(128, 64, 32)\n",
    "\n",
    "        self.conv_cls = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 16, kernel_size=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 2, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        init_weights(self.upconv1.modules())\n",
    "        init_weights(self.upconv2.modules())\n",
    "        init_weights(self.upconv3.modules())\n",
    "        init_weights(self.upconv4.modules())\n",
    "        init_weights(self.conv_cls.modules())\n",
    "\n",
    "    def forward(self, x):\n",
    "        sources = self.basenet(x)\n",
    "        y = torch.cat([sources[0], sources[1]], dim=1)\n",
    "        y = self.upconv1(y)\n",
    "        y = F.interpolate(y, size=sources[2].size()[2:], mode='bilinear', align_corners=False)\n",
    "        y = torch.cat([y, sources[2]], dim=1)\n",
    "        y = self.upconv2(y)\n",
    "        y = F.interpolate(y, size=sources[3].size()[2:], mode='bilinear', align_corners=False)\n",
    "        y = torch.cat([y, sources[3]], dim=1)\n",
    "        y = self.upconv3(y)\n",
    "        y = F.interpolate(y, size=sources[4].size()[2:], mode='bilinear', align_corners=False)\n",
    "        y = torch.cat([y, sources[4]], dim=1)\n",
    "        feature = self.upconv4(y)\n",
    "        y = self.conv_cls(feature)\n",
    "        return y.permute(0, 2, 3, 1), feature\n",
    "\n",
    "def load_weights(model, weight_path):\n",
    "    state_dict = torch.load(weight_path, map_location='cpu')\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:] if k.startswith(\"module.\") else k\n",
    "        new_state_dict[name] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = CRAFT()\n",
    "    weight_path = \"craft_mlt_25k.pth\"\n",
    "    load_weights(model, weight_path)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    output, _ = model(torch.randn(1, 3, 768, 768).cuda())\n",
    "    print(output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from basenet.vgg16_bn import vgg16_bn, init_weights\n",
    "import imgproc\n",
    "import craft_utils\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import google.generativeai as genai\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    def __init__(self, in_ch, mid_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch+mid_ch, mid_ch, kernel_size=1),\n",
    "            nn.BatchNorm2d(mid_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class CRAFT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CRAFT, self).__init__()\n",
    "        self.basenet = vgg16_bn(pretrained=False, freeze=False)\n",
    "        self.upconv1 = double_conv(1024, 512, 256)\n",
    "        self.upconv2 = double_conv(512, 256, 128)\n",
    "        self.upconv3 = double_conv(256, 128, 64)\n",
    "        self.upconv4 = double_conv(128, 64, 32)\n",
    "        self.conv_cls = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 16, kernel_size=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 2, kernel_size=1)\n",
    "        )\n",
    "        init_weights(self.upconv1.modules())\n",
    "        init_weights(self.upconv2.modules())\n",
    "        init_weights(self.upconv3.modules())\n",
    "        init_weights(self.upconv4.modules())\n",
    "        init_weights(self.conv_cls.modules())\n",
    "    def forward(self, x):\n",
    "        sources = self.basenet(x)\n",
    "        y = torch.cat([sources[0], sources[1]], dim=1)\n",
    "        y = self.upconv1(y)\n",
    "        y = F.interpolate(y, size=sources[2].size()[2:], mode='bilinear', align_corners=False)\n",
    "        y = torch.cat([y, sources[2]], dim=1)\n",
    "        y = self.upconv2(y)\n",
    "        y = F.interpolate(y, size=sources[3].size()[2:], mode='bilinear', align_corners=False)\n",
    "        y = torch.cat([y, sources[3]], dim=1)\n",
    "        y = self.upconv3(y)\n",
    "        y = F.interpolate(y, size=sources[4].size()[2:], mode='bilinear', align_corners=False)\n",
    "        y = torch.cat([y, sources[4]], dim=1)\n",
    "        feature = self.upconv4(y)\n",
    "        y = self.conv_cls(feature)\n",
    "        return y.permute(0,2,3,1), feature\n",
    "\n",
    "def load_weights(model, weight_path):\n",
    "    state_dict = torch.load(weight_path, map_location='cpu')\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:] if k.startswith(\"module.\") else k\n",
    "        new_state_dict[name] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "def get_text_color(roi):\n",
    "    roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary = cv2.threshold(roi_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    fg_color = np.mean(roi[binary == 0], axis=0)\n",
    "    return fg_color\n",
    "\n",
    "def noname_for_this_function(words, y_thresh=0.4, x_thres=5.7):\n",
    "    words = [w for w in words if not (w[\"h\"] > 1.5 * w[\"w\"])]\n",
    "    y_centers = np.array([ (w[\"y\"] + w[\"y\"] + w[\"h\"]) / 2 for w in words ]).reshape(-1, 1)\n",
    "    h = np.array([w[\"h\"] for w in words])\n",
    "    med_h = np.median(h)\n",
    "    actual_y_thresh = med_h * y_thresh\n",
    "\n",
    "    if len(words) > 1:\n",
    "        y_clust = AgglomerativeClustering(\n",
    "            n_clusters=None,\n",
    "            distance_threshold=actual_y_thresh,\n",
    "            linkage=\"single\",\n",
    "            compute_full_tree=True\n",
    "        )\n",
    "        y_labels = y_clust.fit_predict(y_centers)\n",
    "    else:\n",
    "        y_labels = np.array([0])\n",
    "\n",
    "    rows = {}\n",
    "    for w, lab in zip(words, y_labels):\n",
    "        rows.setdefault(lab, []).append(w)\n",
    "\n",
    "    row_order = sorted(\n",
    "        rows.keys(),\n",
    "        key=lambda l: np.mean([ (w[\"y\"] + w[\"y\"] + w[\"h\"]) / 2 for w in rows[l] ])\n",
    "    )\n",
    "\n",
    "    sentences = []\n",
    "    for lab in row_order:\n",
    "        row = sorted(rows[lab], key=lambda w: w[\"x\"])\n",
    "        if len(row) == 1:\n",
    "            sentences.append(row)\n",
    "            continue\n",
    "\n",
    "        gaps = [curr[\"x\"] - (prev[\"x\"] + prev[\"w\"])\n",
    "                for prev, curr in zip(row, row[1:])]\n",
    "        positive_gaps = [g for g in gaps if g > 0]\n",
    "        median_gap = np.median(positive_gaps) if positive_gaps else np.median(gaps)\n",
    "        mean_width = np.mean([w[\"w\"] for w in row[:-1]])\n",
    "        threshold_x = max(min(median_gap * x_thres, mean_width * 2), 1)\n",
    "\n",
    "        segment = [row[0]]\n",
    "        for prev, curr, gap in zip(row, row[1:], gaps):\n",
    "            if gap > threshold_x:\n",
    "                sentences.append(segment)\n",
    "                segment = [curr]\n",
    "            else:\n",
    "                segment.append(curr)\n",
    "        sentences.append(segment)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "if __name__=='__main__':\n",
    "    model = CRAFT()\n",
    "    load_weights(model, \"craft_mlt_25k.pth\")\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    image = cv2.imread(\"image.png\")\n",
    "    img_resized, target_ratio, size_heatmap = imgproc.resize_aspect_ratio(image, 1280, interpolation=cv2.INTER_LINEAR, mag_ratio=1.5)\n",
    "    ratio_h = ratio_w = 1/target_ratio\n",
    "    x = imgproc.normalizeMeanVariance(img_resized)\n",
    "    x = torch.from_numpy(x).permute(2,0,1).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        output, _ = model(x)\n",
    "    score_text = output[0,:,:,0].cpu().data.numpy()\n",
    "    score_link = output[0,:,:,1].cpu().data.numpy()\n",
    "    boxes, polys = craft_utils.getDetBoxes(score_text, score_link, 0.7, 0.4, 0.4, False)\n",
    "    boxes = craft_utils.adjustResultCoordinates(boxes, ratio_w, ratio_h)\n",
    "    words = []\n",
    "    for box in boxes:\n",
    "        x_min, y_min = int(min(box, key=lambda p: p[0])[0]), int(min(box, key=lambda p: p[1])[1])\n",
    "        x_max, y_max = int(max(box, key=lambda p: p[0])[0]), int(max(box, key=lambda p: p[1])[1])\n",
    "        roi = image[y_min:y_max, x_min:x_max]\n",
    "        text = pytesseract.image_to_string(roi, config=\"--psm 6\").strip()\n",
    "        if text:\n",
    "            color = get_text_color(roi)\n",
    "            words.append({\"text\": text, \"x\": x_min, \"y\": y_min, \"w\": x_max-x_min, \"h\": y_max-y_min, \"color\": color})\n",
    "    sentences = noname_for_this_function(words)\n",
    "\n",
    "    # Calculate overall bounding box for each sentence\n",
    "    for sentence in sentences:\n",
    "        if sentence:\n",
    "            min_x = min(word[\"x\"] for word in sentence)\n",
    "            min_y = min(word[\"y\"] for word in sentence)\n",
    "            max_x = max(word[\"x\"] + word[\"w\"] for word in sentence)\n",
    "            max_y = max(word[\"y\"] + word[\"h\"] for word in sentence)\n",
    "            sentence[\"bbox\"] = (min_x, min_y, max_x, max_y)\n",
    "\n",
    "    # Collect sentence texts and translate using Gemini API\n",
    "    sentence_texts = [\" \".join(word[\"text\"] for word in sentence) for sentence in sentences]\n",
    "    full_text = \"\\n\".join(sentence_texts)\n",
    "\n",
    "    # Set up Gemini API\n",
    "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Please set the GEMINI_API_KEY environment variable.\")\n",
    "    genai.configure(api_key=api_key)\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "\n",
    "    # Create translation prompt\n",
    "    prompt = (\n",
    "        \"You are a professional translator. \"\n",
    "        \"First, carefully read the entire English text below—each line is separated by newline characters—to fully grasp its context, style, and terminology. \"\n",
    "        \"Then translate it into Vietnamese, producing exactly one Vietnamese line for each English line, and preserve the original line order without skipping, merging, or reordering any lines. \"\n",
    "        \"Return only the translated lines in the exact same order they appeared, separated by newlines, with no additional commentary:\\n\\n\"\n",
    "        f\"{full_text}\"\n",
    "    )\n",
    "\n",
    "    # Translate\n",
    "    response = model.generate_content(prompt)\n",
    "    translated_text = response.text\n",
    "    translated_sentences = translated_text.split(\"\\n\")\n",
    "\n",
    "    # Create mask for all word bounding boxes\n",
    "    all_word_bboxes = []\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            x, y, w, h = word[\"x\"], word[\"y\"], word[\"w\"], word[\"h\"]\n",
    "            all_word_bboxes.append(((x, y), (x + w, y + h)))\n",
    "\n",
    "    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "    for (x1, y1), (x2, y2) in all_word_bboxes:\n",
    "        cv2.rectangle(mask, (x1, y1), (x2, y2), 255, -1)\n",
    "\n",
    "    # Inpaint to remove original text\n",
    "    inpainted_image = cv2.inpaint(image, mask, 3, cv2.INPAINT_TELEA)\n",
    "\n",
    "    # Convert to PIL Image and overlay translated text\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(inpainted_image, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "\n",
    "    # Load font\n",
    "    font_path = \"arial.ttf\"  # Replace with your font file path\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if \"bbox\" in sentence:\n",
    "            min_x, min_y, max_x, max_y = sentence[\"bbox\"]\n",
    "            translated_sentence = translated_sentences[i]\n",
    "            # Set font size based on bounding box height\n",
    "            font_size = int((max_y - min_y) * 0.8)\n",
    "            font_size = max(font_size, 10)  # Minimum font size\n",
    "            font = ImageFont.truetype(font_path, font_size)\n",
    "            draw.text((min_x, min_y), translated_sentence, font=font, fill=(0, 0, 0))\n",
    "\n",
    "    # Save the translated image\n",
    "    pil_image.save(\"translated_image.png\")\n",
    "\n",
    "    # Save original output\n",
    "    img_out = image.copy()\n",
    "    with open(\"xxyy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for sentence in sentences:\n",
    "            text = \" \".join(word[\"text\"] for word in sentence)\n",
    "            x_min, y_min, x_max, y_max = sentence[\"bbox\"]\n",
    "            f.write(f\"{x_min},{y_min},{x_max},{y_max}: {text}\\n\")\n",
    "            cv2.rectangle(img_out, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "    cv2.imwrite(\"result.png\", img_out)\n",
    "\n",
    "    # Save translated text to file\n",
    "    with open(\"translated_xxyy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if \"bbox\" in sentence:\n",
    "                min_x, min_y, max_x, max_y = sentence[\"bbox\"]\n",
    "                translated_sentence = translated_sentences[i]\n",
    "                f.write(f\"{min_x},{y_min},{x_max},{y_max}: {translated_sentence}\\n\")\n",
    "\n",
    "    print(\"Translation complete. Output saved as 'translated_image.png' and 'translated_xxyy.txt'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mywork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
